{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee8630c",
   "metadata": {},
   "source": [
    "# Redundancy with HLS4ML\n",
    "\n",
    "This combines the HLS4ML tutorials 1, 4, and 7a, to showcase an end-to-end demo of building and training the JetTagger model, converting to HLS4ML acclerator, and generating the bitstream with redundancy logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a80f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PROJ_DIR = 'hls4ml_acc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f2380",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63862f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "np.save('X_train_val.npy', X_train_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train_val.npy', y_train_val)\n",
    "np.save('y_test.npy', y_test)\n",
    "np.save('classes.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c75d93",
   "metadata": {},
   "source": [
    "## Construct a model\n",
    "This time we're going to use QKeras layers.\n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "It is maintained by Google and we recently added support for QKeras model to hls4ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeeb71f",
   "metadata": {},
   "source": [
    "We're using `QDense` layer instead of `Dense`, and `QActivation` instead of `Activation`. We're also specifying `kernel_quantizer = quantized_bits(6,0,0)`. This will use 6-bits (of which 0 are integer) for the weights. We also use the same quantization for the biases, and `quantized_relu(6)` for 6-bit ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800575f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    QDense(\n",
    "        64,\n",
    "        input_shape=(16,),\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        32,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        32,\n",
    "        name='fc3',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        5,\n",
    "        name='output',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecde34",
   "metadata": {},
   "source": [
    "## Train sparse\n",
    "Let's train with model sparsity again, since QKeras layers are prunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df9890",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.\n",
    "The callbacks will decay the learning rate and save the model into a directory 'model_2'\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU.\n",
    "If you've restarted the notebook kernel after training once, set `train = False` to load the trained model rather than training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(\n",
    "        stop_patience=1000,\n",
    "        lr_factor=0.5,\n",
    "        lr_patience=10,\n",
    "        lr_epsilon=0.000001,\n",
    "        lr_cooldown=2,\n",
    "        lr_minimum=0.0000001,\n",
    "        outputDir='redundancy',\n",
    "    )\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks.callbacks,\n",
    "    )\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    model = strip_pruning(model)\n",
    "    model.save(TOP_PROJ_DIR+'/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model(TOP_PROJ_DIR+'/KERAS_check_best_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce14d31",
   "metadata": {},
   "source": [
    "## Convert to hls4ml\n",
    "We'll convert our model into `hls4ml`, with a few small changes compared to the previous use of the same model in part 4.\n",
    "We target  `backend='VivadoAccelerator'` backend: this will wrap the HLS NN model, providing an AXI-Stream interface to our IP. We also specify `board='pynq-z2'`.\n",
    "\n",
    "The pynq-z2 board is a popular board with a Zynq 7020 SoC. Since this device is much smaller than the Alveo we specified in previous sections, we set the `ReuseFactor` of all the `Dense` layers of the model to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a76ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "import importlib\n",
    "importlib.reload(hls4ml)\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "for layer in ['fc1', 'fc2', 'fc3', 'output']:\n",
    "    config['LayerName'][layer]['ReuseFactor'] = 64\n",
    "print(\"-----------------------------------\")\n",
    "#plotting.print_dict(config)\n",
    "print(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir=TOP_PROJ_DIR+'/hw', backend='VivadoAccelerator', board='pynq-z2', part='xc7z020clg400-1', clock_period=10\n",
    ")\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e67d4f",
   "metadata": {},
   "source": [
    "We can see which baords are supported in the [documentation](https://fastmachinelearning.org/hls4ml/advanced/accelerator.html). The `VivadoAccelerator` backend introduces the `AcceleratorConfig` section of configuration. Here we can change some details of the interface to the accelerator IP.\n",
    "\n",
    "\n",
    "The `create_initial_config` method (of any backend) can be used to create a template dictionary with the default parameters that you can use as a starting point. In the conversion above, we didn't change any of these settings so all the defaults are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bf01d",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Run the CPU emulation of the hls4ml NN and save the file to compare against the hardware result later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ecbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_test = np.load('X_test.npy')\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))\n",
    "np.save(TOP_PROJ_DIR+'/y_hls.npy', y_hls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af23893",
   "metadata": {},
   "source": [
    "## Synthesize System Design with Accelerator and Redundancy for Pynq\n",
    "./scripts/add_hls4ml.sh && ./scripts/build_hls4ml.sh\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
